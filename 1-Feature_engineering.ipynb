{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "\n",
    "# 自定义py文件\n",
    "from transform import transform_pub\n",
    "from get_78_train_data import get_train_sample, get_train_feature\n",
    "from get_78_test_data import get_test_feature, cleanName\n",
    "from get_sementic_data import get_sementic_feature, generate_embedding_list, calc_sims\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取用于训练的数据\n",
    "+ public：每篇论文的信息\n",
    "+ author_profile: 每个姓名对应的同名作者ID以及发表的论文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public = load_json(\"data/v3/train/train_pub.json\")\n",
    "author_profile = load_json(\"data/v3/train/train_author.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将论文信息的title和abstract去停用词等简单处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not osp.exists(\"data/v3/processed/train_pub.pkl\"):\n",
    "    # 重新生成，可能有点慢\n",
    "    public = transform_pub(public)\n",
    "    dump_json(\"data/v3/processed/train_pub.pkl\", public)\n",
    "else:\n",
    "    # 或者加载现有的\n",
    "    public = load_json(\"data/v3/processed/train_pub.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选取同名人数大于指定值的作者作为训练集\n",
    "negNum = 1\n",
    "paper_and_author = []\n",
    "author_paper_train = {}\n",
    "for name in tqdm(author_profile, desc='sampling'):\n",
    "    if len(author_profile[name]) > negNum:\n",
    "        for person in author_profile[name]:\n",
    "            author_paper_train[person] = deepcopy(author_profile[name][person])\n",
    "            for paper in author_profile[name][person]:\n",
    "                paper_and_author.append((paper, person, name))\n",
    "                \n",
    "print(len(paper_and_author))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手工建模特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为每篇论文采样负样本\n",
    "+  `pos_neg_sample`包括一系列采样的训练样本，每个样本是：paperId, authorId,  name, label\n",
    "+ 如果这篇论文是这个authorid发表的，则label等于1\n",
    "+  如果这篇论文不是这个authorid发表的，但是是与他同名的author发表的，则label等于0\n",
    "\n",
    "## 采样完样本后，自己手动构建特征矩阵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "pos_neg_sample = []\n",
    "for item in tqdm(paper_and_author):\n",
    "    pos_neg_sample.extend(get_train_sample(item, author_profile, negNum=negNum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_feature = []\n",
    "for item in tqdm(pos_neg_sample[:10]):\n",
    "    samples_feature.append(get_train_feature(item, public, author_paper_train))\n",
    "\n",
    "dump_pickle(\"data/v3/processed/samples_feature.pkl\", samples_feature)\n",
    "print(len(samples_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证集数据处理\n",
    "## 加载验证集数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = load_json('data/v3/cna_data/cna_valid_unass.json')\n",
    "val_public = load_json('data/v3/cna_data/cna_valid_unass_pub.json')\n",
    "whole_public = load_json('data/v3/cna_data/whole_author_profiles_pub.json')\n",
    "whole_author_profiles = load_json('data/v3/cna_data/whole_author_profiles.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not osp.exists(\"data/v3/processed/val_pub.pkl\"):\n",
    "    # 重新生成，可能有点慢\n",
    "    val_public = transform_pub(val_public)\n",
    "    dump_json(\"data/v3/processed/val_pub.pkl\", val_public)\n",
    "else:\n",
    "    # 或者加载现有的\n",
    "    val_public = load_json(\"data/v3/processed/val_pub.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_data = defaultdict(list)\n",
    "for item in whole_author_profiles:\n",
    "    name = cleanName(whole_author_profiles[item][\"name\"])\n",
    "    author_data[name].append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIL = []\n",
    "classifySet_all = []\n",
    "candidate_all = []\n",
    "paper_ids = [] \n",
    "\n",
    "for item in tqdm(val_data):\n",
    "    paperId, index = item.split('-')\n",
    "    paperInfo = val_public[paperId]\n",
    "    name = paperInfo[\"authors\"]\n",
    "    name = cleanName(name[int(index)][\"name\"])\n",
    "    \n",
    "    ###### 关键处理部分########################\n",
    "    name = name_remove_comma(name)\n",
    "    name = name_remove_zero(name)\n",
    "    name = ch2en(name)\n",
    "    name = name2name(name)\n",
    "    \n",
    "    candidate = []\n",
    "    candidate = author_data.get(name, [])\n",
    "    if not candidate:\n",
    "        candidate = author_data.get(name_reverse(name), [])\n",
    "        \n",
    "    if not candidate:\n",
    "        candidate = author_data.get(name_reverse(name), [])   \n",
    "        \n",
    "    if not candidate:\n",
    "        candidate_name = find_all_candidate(name, author_data) + find_all_candidate(name_reverse(name), author_data)\n",
    "        if candidate_name:\n",
    "            for c in candidate_name:\n",
    "                candidate.extend(author_data[c])    \n",
    "        \n",
    "    if not candidate:\n",
    "        NIL.append(item)\n",
    "        continue\n",
    "        \n",
    "    ################################################    \n",
    "    \n",
    "    classifySet = []\n",
    "    for personId in candidate:\n",
    "        exam = (paperId, personId) # 用 item的形式取代字符串加'-'连接\n",
    "        temp = get_test_feature(exam, val_public, whole_author_profiles, whole_public)\n",
    "        classifySet.append(temp)\n",
    "\n",
    "    classifySet_all.append(classifySet)\n",
    "    paper_ids.append(paperId)\n",
    "    candidate_all.append(candidate)\n",
    "    \n",
    "print(f'第一次未匹配文章数 {len(NIL)}')\n",
    "\n",
    "dump_pickle(\"data/v3/processed/valid_features.pkl\", classifySet_all)\n",
    "dump_pickle(\"data/v3/processed/valid_paper_ids.pkl\", paper_ids)\n",
    "dump_pickle(\"data/v3/processed/valid_candidate_all.pkl\", candidate_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试集数据处理\n",
    "## 加载测试集数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_json('data/v3/cna_test_data/cna_test_unass.json')\n",
    "test_public = load_json('data/v3/cna_test_data/cna_test_unass_pub.json')\n",
    "whole_public = load_json('data/v3/cna_data/whole_author_profiles_pub.json')\n",
    "whole_author_profiles = load_json('data/v3/cna_data/whole_author_profiles.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not osp.exists(\"data/v3/processed/test_pub.pkl\"):\n",
    "    # 重新生成，可能有点慢\n",
    "    test_public = transform_pub(test_public)\n",
    "    dump_json(\"data/v3/processed/test_pub.pkl\", test_public)\n",
    "else:\n",
    "    # 或者加载现有的\n",
    "    test_public = load_json(\"data/v3/processed/test_pub.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_data = defaultdict(list)\n",
    "for item in whole_author_profiles:\n",
    "    name = cleanName(whole_author_profiles[item][\"name\"])\n",
    "    author_data[name].append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIL = []\n",
    "classifySet_all = []\n",
    "candidate_all = []\n",
    "paper_ids = [] \n",
    "\n",
    "for item in tqdm(test_data):\n",
    "    paperId, index = item.split('-')\n",
    "    paperInfo = test_public[paperId]\n",
    "    name = paperInfo[\"authors\"]\n",
    "    name = cleanName(name[int(index)][\"name\"])\n",
    "    \n",
    "    ###### 关键处理部分########################\n",
    "    name = name_remove_comma(name)\n",
    "    name = name_remove_zero(name)\n",
    "    name = ch2en(name)\n",
    "    name = name2name(name)\n",
    "    \n",
    "    candidate = []\n",
    "    candidate = author_data.get(name, [])\n",
    "    if not candidate:\n",
    "        candidate = author_data.get(name_reverse(name), [])\n",
    "        \n",
    "    if not candidate:\n",
    "        candidate = author_data.get(name_reverse(name), [])   \n",
    "        \n",
    "    if not candidate:\n",
    "        candidate_name = find_all_candidate(name, author_data) + find_all_candidate(name_reverse(name), author_data)\n",
    "        if candidate_name:\n",
    "            for c in candidate_name:\n",
    "                candidate.extend(author_data[c])    \n",
    "        \n",
    "    if not candidate:\n",
    "        NIL.append(item)\n",
    "        continue  \n",
    "        \n",
    "    ################################################    \n",
    "    \n",
    "    classifySet = []\n",
    "    for personId in candidate:\n",
    "        exam = (paperId, personId) # 用 item的形式取代字符串加'-'连接\n",
    "        temp = get_test_feature(exam, test_public, whole_author_profiles, whole_public)\n",
    "        classifySet.append(temp)\n",
    "    \n",
    "\n",
    "    classifySet_all.append(classifySet)\n",
    "    paper_ids.append(paperId)\n",
    "    candidate_all.append(candidate)\n",
    "    \n",
    "print(f'第一次未匹配文章数 {len(NIL)}')\n",
    "\n",
    "dump_pickle(\"data/v3/processed/test_features.pkl\", classifySet_all)\n",
    "dump_pickle(\"data/v3/processed/test_paper_ids.pkl\", paper_ids)\n",
    "dump_pickle(\"data/v3/processed/test_candidate_all.pkl\", candidate_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
