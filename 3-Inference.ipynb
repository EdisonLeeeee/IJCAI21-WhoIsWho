{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from sklearn import model_selection, metrics\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# 自定义py文件\n",
    "from transform import transform_pub\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_splits = 5\n",
    "seed = 0\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载已经提取过特征的test数据\n",
    "\n",
    "candidates_path = \"data/v3/processed/valid_candidate_all.pkl\"\n",
    "paper_ids_path = \"data/v3/processed/valid_paper_ids.pkl\"\n",
    "valid_features_path = \"data/v3/processed/valid_features.pkl\"\n",
    "\n",
    "# candidates_path = \"data/v3/processed/test_candidate_all.pkl\"\n",
    "# paper_ids_path = \"data/v3/processed/test_paper_ids.pkl\"\n",
    "# valid_features_path = \"data/v3/processed/test_features.pkl\"\n",
    "\n",
    "candidates = load_pickle(candidates_path)\n",
    "paper_ids = load_pickle(paper_ids_path)\n",
    "valid_features = load_pickle(valid_features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for arr in valid_features:\n",
    "    indices.append(int(np.array(arr).shape[0]))\n",
    "\n",
    "new_valid_features = np.vstack(valid_features)\n",
    "print(new_valid_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt_filename = \"data/v3/processed/qt.pkl\"\n",
    "qt = load_pickle(qt_filename)\n",
    "\n",
    "# 按方差或最终效果剔除某些特征 78 -6 =72\n",
    "remove_features = [0,2,20,24,44,76]\n",
    "features = np.delete(new_valid_features, remove_features, axis=1)\n",
    "# 这里用QuantileTransformer来进行scaling\n",
    "transformed_features = qt.transform(features)\n",
    "\n",
    "# 根据相关性和特征重要性进行特征交叉 16\n",
    "cross_features = np.array([\n",
    "    transformed_features[:,36] * transformed_features[:,10],\n",
    "    transformed_features[:,1] * transformed_features[:,5],\n",
    "    transformed_features[:,4] * transformed_features[:,6],\n",
    "    transformed_features[:,35] * transformed_features[:,32],\n",
    "    transformed_features[:,37] * transformed_features[:,39],\n",
    "    transformed_features[:,3] * transformed_features[:,8],\n",
    "    transformed_features[:,50] * transformed_features[:,52],\n",
    "    transformed_features[:,40] * transformed_features[:,38],\n",
    "    transformed_features[:,40] * transformed_features[:,41],\n",
    "    transformed_features[:,50] * transformed_features[:,53],\n",
    "    transformed_features[:,41] * transformed_features[:,34],\n",
    "    transformed_features[:,7] * transformed_features[:,8],\n",
    "    transformed_features[:,34] * transformed_features[:,37],\n",
    "    transformed_features[:,22] * transformed_features[:,23],\n",
    "    transformed_features[:,3] * transformed_features[:,7],\n",
    "    transformed_features[:,18] * transformed_features[:,10],\n",
    "])\n",
    "\n",
    "# 72 + 16 = 88\n",
    "valid_features = np.hstack([transformed_features, cross_features.T])\n",
    "valid_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化\n",
    "mean_path = \"data/v3/processed/mean.pkl\"\n",
    "std_path = \"data/v3/processed/std.pkl\"\n",
    "\n",
    "mean = load_pickle(mean_path)\n",
    "std = load_pickle(std_path)\n",
    "\n",
    "valid_features = (valid_features - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_save_path = \"data/v3/processed/models/\"\n",
    "\n",
    "result_dict = {\"rf\":defaultdict(list), \"xgb\":defaultdict(list), \"lgbm\":defaultdict(list), \"cat\":defaultdict(list), \"ensemble\":defaultdict(list)}\n",
    "\n",
    "rf_prob = 0.0\n",
    "xgb_prob = 0.0\n",
    "lgbm_prob = 0.0\n",
    "cat_prob = 0.0\n",
    "\n",
    "\n",
    "for i in tqdm(range(N_splits)):\n",
    "    rf = pickle.load(open(model_save_path + f\"model_{i}_rf.dat\", \"rb\"))\n",
    "    rf_prob += rf.predict_proba(valid_features)[:, 1] / N_splits\n",
    "\n",
    "for i in tqdm(range(N_splits)):\n",
    "    xgb = pickle.load(open(model_save_path + f\"model_{i}_xgb.dat\", \"rb\"))\n",
    "    xgb_prob += xgb.predict_proba(valid_features)[:, 1] / N_splits\n",
    "\n",
    "for i in tqdm(range(N_splits)):\n",
    "    lgbm = pickle.load(open(model_save_path + f\"model_{i}_lgbm.dat\", \"rb\"))\n",
    "    lgbm_prob += lgbm.predict_proba(valid_features)[:, 1] / N_splits\n",
    "\n",
    "for i in tqdm(range(N_splits)):\n",
    "    cat = pickle.load(open(model_save_path + f\"model_{i}_cat.dat\", \"rb\"))\n",
    "    cat_prob += cat.predict_proba(valid_features)[:, 1] / N_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "tmp_idx = 0\n",
    "for num, idx in enumerate(tqdm(indices)):\n",
    "\n",
    "    rf_prob_tmp = rf_prob[tmp_idx: (tmp_idx + idx)]\n",
    "    xgb_prob_tmp = xgb_prob[tmp_idx: (tmp_idx + idx)]\n",
    "    lgbm_prob_tmp = lgbm_prob[tmp_idx: (tmp_idx + idx)]\n",
    "    cat_prob_tmp = cat_prob[tmp_idx: (tmp_idx + idx)]\n",
    "    \n",
    "    rf_idx = np.argmax(np.array(rf_prob_tmp))\n",
    "    xgb_idx = np.argmax(np.array(xgb_prob_tmp))\n",
    "    lgbm_idx = np.argmax(np.array(lgbm_prob_tmp))\n",
    "    cat_idx = np.argmax(np.array(cat_prob_tmp))\n",
    "    \n",
    "    ensemble_prob = rf_prob_tmp + xgb_prob_tmp + lgbm_prob_tmp + cat_prob_tmp\n",
    "    ensemble_idx = np.argmax(np.array(ensemble_prob))\n",
    "\n",
    "\n",
    "    result_dict[\"rf\"][candidates[num][rf_idx]].append((paper_ids[num], np.array(rf_prob_tmp)[rf_idx]))\n",
    "    result_dict[\"xgb\"][candidates[num][xgb_idx]].append((paper_ids[num], np.array(xgb_prob_tmp)[xgb_idx]))\n",
    "    result_dict[\"lgbm\"][candidates[num][lgbm_idx]].append((paper_ids[num], np.array(lgbm_prob_tmp)[lgbm_idx]))\n",
    "    result_dict[\"cat\"][candidates[num][cat_idx]].append((paper_ids[num], np.array(cat_prob_tmp)[lgbm_idx]))\n",
    "    result_dict[\"ensemble\"][candidates[num][ensemble_idx]].append((paper_ids[num], np.array(ensemble_prob)[ensemble_idx]))\n",
    "    \n",
    "    tmp_idx += idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# 多阈值设定\n",
    "thresholds = [0.6, 0.7, 0.8]\n",
    "models = ['rf', 'xgb', 'lgbm', 'cat', \"ensemble\"]\n",
    "num_dicision_votes = 8  # 3 * 5 / 2 = 8\n",
    "\n",
    "result_votes = defaultdict(int)\n",
    "result = defaultdict(list)\n",
    "\n",
    "#多阈值处理 -> (作者ID, paperId) : 投票数\n",
    "                    \n",
    "for model in models:\n",
    "    num = 0\n",
    "    for items in result_dict[model]:  # 该模型搜索出的作者名字\n",
    "        for item in result_dict[model][items]: # 作者名字对应的作者ID\n",
    "            paperId, prob = item\n",
    "            for threshold in thresholds: # 多阈值\n",
    "                if prob >= threshold:\n",
    "                    result_votes[(items,paperId)] += 1\n",
    "\n",
    "# 生成提交结果\n",
    "for item in result_votes.keys():\n",
    "    tmp_votes = result_votes[item]\n",
    "    if tmp_votes >= num_dicision_votes:\n",
    "        result[item[0]].append(item[1])\n",
    "        \n",
    "        \n",
    "\n",
    "# 导出结果  \n",
    "dump_json(\"vote_result_all.json\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
